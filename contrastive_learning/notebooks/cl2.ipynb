{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:32:17.270043Z","iopub.status.busy":"2023-06-24T09:32:17.269057Z","iopub.status.idle":"2023-06-24T09:32:17.278593Z","shell.execute_reply":"2023-06-24T09:32:17.277693Z","shell.execute_reply.started":"2023-06-24T09:32:17.270009Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import keras\n","import pickle\n","import random\n","import imageio\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import tensorflow as tf\n","import matplotlib.cm as cm\n","from keras import regularizers\n","import matplotlib.pyplot as plt\n","from keras.utils import Sequence\n","from keras.optimizers import Adam\n","from keras import optimizers, metrics\n","from keras.utils import to_categorical\n","from keras.callbacks import ReduceLROnPlateau\n","from keras import layers, models, applications\n","from tensorflow_addons.activations import gelu\n","from sklearn.preprocessing import label_binarize\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from keras.models import load_model, Model, Sequential\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers.experimental import preprocessing\n","from tensorflow.keras.applications import ResNet50, EfficientNetB1\n","from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score, roc_auc_score, classification_report\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, GlobalAveragePooling2D, PReLU, Reshape, BatchNormalization, Dropout\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T19:02:50.631054Z","iopub.status.busy":"2023-06-23T19:02:50.630136Z","iopub.status.idle":"2023-06-23T19:02:54.200920Z","shell.execute_reply":"2023-06-23T19:02:54.200101Z","shell.execute_reply.started":"2023-06-23T19:02:50.631010Z"},"trusted":true},"outputs":[],"source":["fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(6, 4))\n","\n","for i, (folder_path, title) in enumerate([\n","    ('Model_II/no_sub', 'No Substructure'),\n","    ('Model_II/axion', 'Axion'),\n","    ('Model_II/cdm', 'Cold Dark Matter (CDM)')]):\n","    file_name = os.listdir(folder_path)[7]\n","    file_path = os.path.join(folder_path, file_name)\n","    loaded_file = np.load(file_path, allow_pickle=True)\n","    print(f'Shape of {title}: {loaded_file.shape}') # Since the dataset states that 'Axion files have extra data corresponding to mass of axion used in simulation'. So the .npy file has a shape of 2, having the image and this mass.\n","\n","    if (folder_path == 'Model_II/axion'):\n","        img_array = loaded_file[0]\n","    else:\n","        img_array = loaded_file\n","    axs[i].imshow(img_array, cmap='inferno')\n","    axs[i].set_title(title)\n","    axs[i].set_xticks([])\n","    axs[i].set_yticks([])\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:32:22.160020Z","iopub.status.busy":"2023-06-24T09:32:22.159578Z","iopub.status.idle":"2023-06-24T09:32:22.172384Z","shell.execute_reply":"2023-06-24T09:32:22.171184Z","shell.execute_reply.started":"2023-06-24T09:32:22.159981Z"},"trusted":true},"outputs":[],"source":["# Algorithm hyperparameters\n","input_shape = (64, 64, 3)\n","num_epochs = 20\n","batch_size = 256 \n","width = 256\n","temperature = 0.1\n","queue_size = 10000\n","\n","contrastive_augmentation_rotation = {\"factor\": 0.5,\"name\": \"contrastive_augmenter_rotation\"}\n","contrastive_augmentation_gaussian = {\"std_dev\": 0.1,\"name\": \"contrastive_augmenter_gaussian\"} # Std_dev = 0.04 for rgb and 0.001 for 1 channel\n","\n","\n","# classification_augmentation = {\"brightness\": 0.1,\"name\": \"classification_augmenter\",\"scale\": (0.5, 3.0),}\n","\n","# total_samples = sum([len(files) for r, d, files in os.walk(data_path)])\n","# steps_per_epoch = total_samples // batch_size\n","# validation_steps = total_samples // batch_size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:32:24.048452Z","iopub.status.busy":"2023-06-24T09:32:24.048044Z","iopub.status.idle":"2023-06-24T09:32:26.400188Z","shell.execute_reply":"2023-06-24T09:32:26.399240Z","shell.execute_reply.started":"2023-06-24T09:32:24.048420Z"},"trusted":true},"outputs":[],"source":["classes = ['axion', 'cdm', 'no_sub']\n","class_indices = {class_name: idx for idx, class_name in enumerate(classes)}\n","\n","def image_generator(files, label_type, batch_size = batch_size):\n","    while True:\n","        # Select files (paths/indices) for the batch\n","        batch_paths = np.random.choice(a = files, size = batch_size)\n","        batch_input = []\n","        batch_output = []\n","\n","        # Read in each input, perform preprocessing and get labels\n","        for input_path in batch_paths:\n","            class_name = os.path.basename(os.path.dirname(input_path))\n","            label = class_indices[class_name] \n","            if class_name == 'axion':\n","                input = np.load(input_path, allow_pickle=True)[0][..., np.newaxis]\n","            else:\n","                input = np.load(input_path, allow_pickle=False)[..., np.newaxis]\n","                \n","            img_3d = np.repeat(input, 3, axis=-1) # For RGB channel\n","            img_3d  = (img_3d - np.min(img_3d)) / (np.max(img_3d) - np.min(img_3d)) # For RGB channel\n","            batch_input += [ img_3d ] # For RGB channel\n","#             batch_input += [ input ] # For Single channel\n","            batch_output += [ label ]\n","    \n","        batch_x = np.array(batch_input)\n","        if label_type == 'one_hot':\n","            batch_y = to_categorical(batch_output, num_classes=3)  # Perform one-hot encoding\n","        elif label_type == 'label':\n","            batch_y = np.array(batch_output)\n","        # print(batch_x.shape,batch_y.shape)\n","        yield(batch_x, batch_y)\n","\n","# Get all file paths in the data_path\n","data_path = 'Model_II/' \n","file_paths = []\n","for class_name in classes:\n","    class_dir = os.path.join(data_path, class_name)\n","    file_paths += [os.path.join(class_dir, file) for file in os.listdir(class_dir)]\n","\n","\n","train_files, test_files = train_test_split(file_paths, test_size=0.1, random_state=69)\n","\n","train_generator_one_hot = image_generator(train_files, label_type = 'one_hot', batch_size=batch_size)\n","test_generator_one_hot = image_generator(test_files, label_type = 'one_hot', batch_size=batch_size)\n","\n","train_generator_label = image_generator(train_files,label_type = 'label',  batch_size=batch_size)\n","test_generator_label = image_generator(test_files,label_type = 'label',  batch_size=batch_size)\n","\n","total_samples = len(train_files)\n","steps_per_epoch = total_samples // batch_size\n","total_validation_samples = len(test_files)\n","validation_steps = total_validation_samples // batch_size"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T07:24:35.339378Z","iopub.status.busy":"2023-06-23T07:24:35.338950Z","iopub.status.idle":"2023-06-23T07:24:37.757064Z","shell.execute_reply":"2023-06-23T07:24:37.755690Z","shell.execute_reply.started":"2023-06-23T07:24:35.339341Z"},"trusted":true},"outputs":[],"source":["image_sample, label_sample = next(train_generator_one_hot)\n","print(f'Array Range: {np.min(image_sample[0])}, {np.max(image_sample[0])}')\n","print(f'Sample Image of label {\"axion\" if np.argmax(label_sample[0]) == 0 else \"cdm\" if np.argmax(label_sample[0]) == 1 else \"no_sub\"}: ')\n","plt.figure(figsize=(4,4))\n","plt.imshow(np.mean(image_sample[0], axis=2), cmap='inferno') # np.mean() is used to convert the rgb image to greyscale so we can apply cmap to it.\n","plt.axis('off')\n","plt.show()\n","print(f'Image shape: {image_sample.shape}\\nLabel shape: {label_sample.shape}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:32:27.367401Z","iopub.status.busy":"2023-06-24T09:32:27.366559Z","iopub.status.idle":"2023-06-24T09:32:27.392614Z","shell.execute_reply":"2023-06-24T09:32:27.391442Z","shell.execute_reply.started":"2023-06-24T09:32:27.367358Z"},"trusted":true},"outputs":[],"source":["class RandomResizedCrop(layers.Layer):\n","    def __init__(self, scale, ratio):\n","        super().__init__()\n","        self.scale = scale\n","        self.log_ratio = (tf.math.log(ratio[0]), tf.math.log(ratio[1]))\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        height = tf.shape(images)[1]\n","        width = tf.shape(images)[2]\n","\n","        random_scales = tf.random.uniform((batch_size,), self.scale[0], self.scale[1])\n","        random_ratios = tf.exp(\n","            tf.random.uniform((batch_size,), self.log_ratio[0], self.log_ratio[1])\n","        )\n","\n","        new_heights = tf.clip_by_value(tf.sqrt(random_scales / random_ratios), 0, 1)\n","        new_widths = tf.clip_by_value(tf.sqrt(random_scales * random_ratios), 0, 1)\n","        height_offsets = tf.random.uniform((batch_size,), 0, 1 - new_heights)\n","        width_offsets = tf.random.uniform((batch_size,), 0, 1 - new_widths)\n","\n","        bounding_boxes = tf.stack(\n","            [\n","                height_offsets,\n","                width_offsets,\n","                height_offsets + new_heights,\n","                width_offsets + new_widths,\n","            ],\n","            axis=1,\n","        )\n","        images = tf.image.crop_and_resize(\n","            images, bounding_boxes, tf.range(batch_size), (height, width)\n","        )\n","        return images\n","    \n","class RandomGaussianNoise(layers.Layer):\n","    def __init__(self, std_dev):\n","        super().__init__()\n","        self.std_dev = std_dev\n","\n","    def add_gaussian_noise(self, images):\n","        noise = tf.random.normal(shape=tf.shape(images), mean=0., stddev=self.std_dev)\n","        return tf.clip_by_value(images + noise, 0, 1)\n","\n","    def call(self, images):\n","        images = self.add_gaussian_noise(images)\n","        return images\n","\n","    \n","    \n","def get_augmenter_only_gaussian(std_dev, name):\n","    return keras.Sequential(\n","        [\n","            layers.Input(shape=input_shape),\n","            # layers.Rescaling(1 / 255), # Only for Single Channel Images\n","#             layers.RandomRotation(1), # Not needed for now\n","            RandomGaussianNoise(std_dev=std_dev),\n","#             RandomResizedCrop(scale=scale, ratio=(3 / 4, 4 / 3)),\n","#             RandomBrightness(brightness=brightness),\n","        ],\n","        name=name,)\n","\n","def get_augmenter_only_rotation(factor, name):\n","    return keras.Sequential(\n","        [\n","            layers.Input(shape=input_shape),\n","            layers.RandomRotation(factor), \n","        ],\n","        name=name,)\n","\n","class RandomBrightness(layers.Layer):\n","    def __init__(self, brightness):\n","        super().__init__()\n","        self.brightness = brightness\n","    def blend(self, images_1, images_2, ratios):\n","        return tf.clip_by_value(ratios * images_1 + (1.0 - ratios) * images_2, 0, 1)\n","    def random_brightness(self, images):\n","        # random interpolation/extrapolation between the image and darkness\n","        return self.blend(\n","            images,\n","            0,\n","            tf.random.uniform(\n","                (tf.shape(images)[0], 1, 1, 1), 1 - self.brightness, 1 + self.brightness),)\n","    def call(self, images):\n","        images = self.random_brightness(images)\n","        return images "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T19:25:41.745056Z","iopub.status.busy":"2023-06-23T19:25:41.744663Z","iopub.status.idle":"2023-06-23T19:25:45.886126Z","shell.execute_reply":"2023-06-23T19:25:45.885298Z","shell.execute_reply.started":"2023-06-23T19:25:41.745024Z"},"trusted":true},"outputs":[],"source":["def visualize_augmentations(num_images):\n","    # Sample a batch from a dataset\n","    images = next(train_generator_one_hot)[0][:num_images]\n","    augmented_images = zip(\n","        images,\n","        get_augmenter_only_rotation(**contrastive_augmentation_rotation)(images),\n","        get_augmenter_only_gaussian(**contrastive_augmentation_gaussian)(images),\n","    )\n","    row_titles = [\n","        \"Original:\",\n","        \"Rotational Augmentation:\",\n","        \"Gaussian Noise Augmentation:\",\n","    ]\n","    plt.figure(figsize=(18, 6), dpi=120)\n","    for column, image_row in enumerate(augmented_images):\n","        for row, image in enumerate(image_row):\n","            plt.subplot(3, num_images, row * num_images + column + 1)\n","            plt.imshow(np.mean(image, axis=2), cmap='inferno')\n","            if column == 0:\n","                plt.title(row_titles[row], loc=\"left\")\n","            plt.axis(\"off\")\n","\n","visualize_augmentations(num_images=7)"]},{"cell_type":"markdown","metadata":{},"source":["### Baseline Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def rotate_filters(filters):\n","    # Rotate by 90, 180, and 270 degrees\n","    filters_90 = tf.image.rot90(filters, k=1)\n","    filters_180 = tf.image.rot90(filters, k=2)\n","    filters_270 = tf.image.rot90(filters, k=3)\n","    return [filters, filters_90, filters_180, filters_270]\n","\n","class EquivariantConv2D(tf.keras.layers.Layer):\n","    def __init__(self, num_filters, kernel_size, **kwargs):\n","        super(EquivariantConv2D, self).__init__(**kwargs)\n","        self.num_filters = num_filters\n","        self.kernel_size = kernel_size\n","\n","    def build(self, input_shape):\n","        self.filters = self.add_weight(\n","            \"filters\",\n","            shape=[self.kernel_size, self.kernel_size, input_shape[-1], self.num_filters],\n","            initializer='glorot_uniform',\n","            trainable=True,\n","        )\n","\n","    def call(self, inputs):\n","        # Generate rotated versions of the filters\n","        rotated_filters = rotate_filters(self.filters)\n","        \n","        # Perform convolutions with each set of rotated filters\n","        outputs = [tf.nn.conv2d(inputs, filters, strides=[1, 1, 1, 1], padding='SAME') for filters in rotated_filters]\n","        \n","        # Concatenate along the feature dimension\n","        output = tf.concat(outputs, axis=-1)\n","        return output\n","\n","    def get_config(self):\n","        config = super().get_config().copy()\n","        config.update({\n","            'num_filters': self.num_filters,\n","            'kernel_size': self.kernel_size,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Dummy_Input(tf.keras.layers.Layer):\n","    \"\"\"\n","    This is the class for a 'classification token' mentioned in the ViT Paper\n","    \n","    @inproceedings{50650,\n","    title\t= {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n","    author\t= {Alexander Kolesnikov and Alexey Dosovitskiy and Dirk Weissenborn and Georg Heigold and Jakob Uszkoreit and Lucas Beyer and Matthias Minderer and Mostafa Dehghani and Neil Houlsby and Sylvain Gelly and Thomas Unterthiner and Xiaohua Zhai},\n","    year\t= {2021}\n","    }\n","\n","    This adds a dummy input to an input layer which acts as a placeholder data structure that’s used to store information \n","    that is extracted from other tokens in the sequence. \n","    \n","    By allocating an empty token for this procedure, it seems like the Vision Transformer makes \n","    it less likely to bias the final output towards or against any single one of the other individual tokens.  \n","    \"\"\"\n","    def build(self, input_dimensions):\n","        self.dummy_val = self.add_weight(name=\"dummy_input\",\n","                                         shape=(1, 1, input_dimensions[-1]), \n","                                         initializer=tf.zeros_initializer(),\n","                                         trainable=True,)\n","\n","    def call(self, inputs):\n","        dummy_val_token = tf.tile(self.dummy_val, \n","                                  [tf.shape(inputs)[0], 1, 1])\n","        return tf.concat([dummy_val_token, \n","                          inputs], axis=1)\n","\n","class Inherit_Positional_Embeddings(tf.keras.layers.Layer):\n","    \"\"\"\n","    In Vision Transformers, the input data is typically a 2D image that is flattened into a sequence of patches, which are then fed into the transformer model. \n","    Since transformers do not inherently model positional information, additional positional embeddings are added to the input sequence in this code \n","    to enable the model to take into account the spatial relationships between the patches.\n","    \"\"\"\n","\n","    def build(self, input_shape):\n","        self.learned_positional_embeddings = tf.Variable(name=\"pos_embedding\",\n","                                                         initial_value=tf.initializers.random_normal(stddev=0.06)(shape=(1, input_shape[1], input_shape[2])),\n","                                                         dtype=\"float32\", \n","                                                         trainable=True,)\n","    def call(self, inputs):\n","        return inputs + self.learned_positional_embeddings\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    \"\"\"\n","    From, \n","        \n","    @inproceedings{50650,\n","    title\t= {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n","    author\t= {Alexander Kolesnikov and Alexey Dosovitskiy and Dirk Weissenborn and Georg Heigold and Jakob Uszkoreit and Lucas Beyer and Matthias Minderer and Mostafa Dehghani and Neil Houlsby and Sylvain Gelly and Thomas Unterthiner and Xiaohua Zhai},\n","    year\t= {2021}\n","    }\n","    \n","    Multihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\n","    called “heads”, in parallel, and project their concatenated outputs. \n","    \n","    MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)]Umsa  \n","    where, Umsa ∈ R k·Dh×D is a learnable weight matrix. Credits to faustomorales the author or vit-keras for simplification of MultiHeadSelfAttention in his library.\n","    \n","    \"\"\"\n","    def __init__(self, *args, num_heads, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.num_heads = num_heads\n","\n","    def build(self, input_shape):\n","        hidden_size = input_shape[-1]\n","        num_heads = self.num_heads\n","        self.hidden_size = hidden_size\n","        self.projection_dim = hidden_size // num_heads\n","        self.query = tf.keras.layers.Dense(hidden_size, name=\"query\")\n","        self.key = tf.keras.layers.Dense(hidden_size, name=\"key\")\n","        self.value = tf.keras.layers.Dense(hidden_size, name=\"value\")\n","        self.combine_heads = tf.keras.layers.Dense(hidden_size, name=\"out\")\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], score.dtype)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output, weights\n","\n","    def call(self, inputs):\n","        batch_size = tf.shape(inputs)[0]\n","        query = self.query(inputs)\n","        key = self.key(inputs)\n","        value = self.value(inputs)\n","        query = tf.transpose(tf.reshape(query, (batch_size, -1, self.num_heads, self.projection_dim)), perm=[0, 2, 1, 3])\n","        key = tf.transpose(tf.reshape(key, (batch_size, -1, self.num_heads, self.projection_dim)), perm=[0, 2, 1, 3])\n","        value = tf.transpose(tf.reshape(value, (batch_size, -1, self.num_heads, self.projection_dim)), perm=[0, 2, 1, 3])\n","\n","        attention, weights = self.attention(query, key, value)\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(attention, (batch_size, -1, self.hidden_size))\n","        output = self.combine_heads(concat_attention)\n","        return output, weights\n","\n","\n","class TransformerBlock(tf.keras.layers.Layer):\n","    \"\"\"Implements a Transformer block.\"\"\"\n","\n","    def __init__(self, *args, num_heads, mlp_dim, dropout, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.num_heads = num_heads\n","        self.mlp_dim = mlp_dim\n","        self.dropout = dropout\n","\n","    def build(self, input_shape):\n","        self.att = MultiHeadSelfAttention(\n","            num_heads=self.num_heads,\n","            name=\"MultiHeadDotProductAttention_1\",\n","        )\n","        self.mlpblock = tf.keras.Sequential(\n","            [\n","                tf.keras.layers.Dense(\n","                    self.mlp_dim,\n","                    activation=\"linear\",\n","                    name=f\"{self.name}/Dense_0\",\n","                ),\n","                tf.keras.layers.Lambda(\n","                    lambda x: gelu(x, approximate=False)\n","                )\n","                if hasattr(tf.keras.activations, \"gelu\")\n","                else tf.keras.layers.Lambda(\n","                    lambda x: gelu(x, approximate=False)\n","                ),\n","                tf.keras.layers.Dropout(self.dropout),\n","                tf.keras.layers.Dense(input_shape[-1], name=f\"{self.name}/Dense_1\"),\n","                tf.keras.layers.Dropout(self.dropout),\n","            ],\n","            name=\"MlpBlock_3\",\n","        )\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(\n","            epsilon=1e-6, name=\"LayerNorm_0\"\n","        )\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(\n","            epsilon=1e-6, name=\"LayerNorm_2\"\n","        )\n","        self.dropout_layer = tf.keras.layers.Dropout(self.dropout)\n","\n","    def call(self, inputs, training):\n","        x = self.layernorm1(inputs)\n","        x, weights = self.att(x)\n","        x = self.dropout_layer(x, training=training)\n","        x = x + inputs\n","        y = self.layernorm2(x)\n","        y = self.mlpblock(y)\n","        return x + y, weights\n","\n","def load_ViT_model_with_no_weights(): \n","    x = Input(shape=(64, 64, 3)) \n","    y = Conv2D(filters=768, kernel_size=32,strides=32,padding=\"valid\", name=\"embedding\",)(x)\n","    y = Reshape((y.shape[1] * y.shape[2], 768))(y)\n","    # y = Reshape((5, 768))(y)\n","\n","    y = Dummy_Input(name=\"dummy_input\")(y)\n","    y = Inherit_Positional_Embeddings(name=\"Transformer/posembed_input\")(y)\n","    \n","    for n in range(12):\n","        y, _ = TransformerBlock(\n","            num_heads=12,\n","            mlp_dim=3072,\n","            dropout=0.1,\n","            name=f\"Transformer/encoderblock_{n}\",)(y)\n","    y = tf.keras.layers.LayerNormalization(\n","        epsilon=1e-6, name=\"Transformer/encoder_norm\")(y)\n","    y = tf.keras.layers.Lambda(lambda v: v[:, 0], name=\"ExtractToken\")(y)\n","    y = Dense(768, name=\"pre_logits\", activation=\"tanh\")(y)\n","    model = Model(inputs=x, outputs=y)\n","    return model\n","\n","load_vit = load_ViT_model_with_no_weights()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:32:34.010153Z","iopub.status.busy":"2023-06-24T09:32:34.009157Z","iopub.status.idle":"2023-06-24T09:32:42.698116Z","shell.execute_reply":"2023-06-24T09:32:42.696897Z","shell.execute_reply.started":"2023-06-24T09:32:34.010119Z"},"trusted":true},"outputs":[],"source":["def get_encoder():\n","    top_layers = tf.keras.models.Sequential([ \n","        BatchNormalization(),\n","        Dense(32, activation = gelu),\n","        Dropout(0.2),\n","        BatchNormalization(),\n","        Dense(16, activation = gelu),\n","        BatchNormalization(),\n","    ], name='modified_vit_top')\n","    model = Sequential([\n","        load_vit,\n","        top_layers\n","    ], name='modified_vit')\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["baseline_model = keras.Sequential(\n","    [ \n","        get_encoder(),\n","        layers.Dense(3, activation='softmax'),\n","    ],\n","    name=\"baseline_model\",\n",")\n","baseline_model.compile(\n","    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate= 1e-4),\n","    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n","    metrics=['acc', metrics.AUC(name='auc')])\n","\n","baseline_history = baseline_model.fit(train_generator_one_hot, steps_per_epoch=steps_per_epoch, epochs=num_epochs, \n","                                      validation_data=test_generator_one_hot, validation_steps = validation_steps)\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(baseline_history.history[\"val_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T09:14:26.958136Z","iopub.status.busy":"2023-06-23T09:14:26.957191Z","iopub.status.idle":"2023-06-23T10:24:00.966026Z","shell.execute_reply":"2023-06-23T10:24:00.964792Z","shell.execute_reply.started":"2023-06-23T09:14:26.958090Z"},"trusted":true},"outputs":[],"source":["baseline_model = keras.Sequential(\n","    [ \n","        get_encoder(),\n","        layers.Dense(3, activation='softmax'),\n","    ],\n","    name=\"baseline_model\",\n",")\n","baseline_model.compile(\n","    optimizer=keras.optimizers.Adam(learning_rate= 1e-4),\n","    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n","    metrics=['acc', metrics.AUC(name='auc')])\n","\n","baseline_history = baseline_model.fit(train_generator_one_hot, steps_per_epoch=steps_per_epoch, epochs=num_epochs, \n","                                      validation_data=test_generator_one_hot, validation_steps = validation_steps)\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(baseline_history.history[\"val_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T10:24:00.968233Z","iopub.status.busy":"2023-06-23T10:24:00.967896Z","iopub.status.idle":"2023-06-23T10:24:01.490474Z","shell.execute_reply":"2023-06-23T10:24:01.489460Z","shell.execute_reply.started":"2023-06-23T10:24:00.968205Z"},"trusted":true},"outputs":[],"source":["baseline_model.save_weights('baseline_model_resnet50.h5')\n","# loaded_model = ContrastiveModel()\n","# pretraining_model.load_weights('pretraining_model.h5')\n","with open('baseline_history_resnet50.pkl', 'wb') as file:\n","    pickle.dump(baseline_history.history, file)"]},{"cell_type":"markdown","metadata":{},"source":["### Self-Supervised Contrastive Pretraining\n","\n","\n","c_loss: Contrastive loss. It is a measure of dissimilarity between pairs of augmented images. This loss is calculated using the InfoNCE (Information Noise-Contrastive Estimation) or NT-Xent (Normalized Temperature-Scaled Cross Entropy) loss, which encourages similar representations for augmented views of the same image and dissimilar representations for different images.\n","\n","c_acc: Contrastive accuracy. It measures the accuracy of the contrastive predictions. In this case, it represents how well the model is able to distinguish between the representations of augmented views of the same image and different images.\n","\n","p_loss: Probe loss. It is the loss for the linear probe, which is used for evaluation and downstream classification tasks. The probe loss is calculated using the sparse categorical cross-entropy loss, comparing the predicted class probabilities with the true labels.\n","\n","p_acc: Probe accuracy. It represents the accuracy of the linear probe, indicating how well the model performs on the downstream classification task."]},{"cell_type":"markdown","metadata":{},"source":["### With only Rotational"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T15:41:13.274344Z","iopub.status.busy":"2023-06-23T15:41:13.273952Z","iopub.status.idle":"2023-06-23T16:52:09.419843Z","shell.execute_reply":"2023-06-23T16:52:09.418882Z","shell.execute_reply.started":"2023-06-23T15:41:13.274313Z"},"trusted":true},"outputs":[],"source":["# Define the contrastive model with model-subclassing\n","class ContrastiveModel(keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.temperature = temperature\n","        self.contrastive_augmenter = get_augmenter_only_rotation(**contrastive_augmentation_rotation)\n","        self.encoder = get_encoder()\n","        # Non-linear MLP as projection head\n","        self.projection_head = keras.Sequential(\n","            [\n","                keras.Input(shape=(width,)),\n","                layers.Dense(width, activation=\"relu\"),\n","                layers.Dense(width),\n","            ],\n","            name=\"projection_head\",\n","        )\n","        # Single dense layer for linear probing\n","        self.linear_probe = keras.Sequential(\n","            [layers.Input(shape=(width,)), layers.Dense(3)], name=\"linear_probe\"\n","        )\n","\n","        self.encoder.summary()\n","        self.projection_head.summary()\n","        self.linear_probe.summary()\n","\n","    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n","        super().compile(**kwargs)\n","\n","        self.contrastive_optimizer = contrastive_optimizer\n","        self.probe_optimizer = probe_optimizer\n","\n","        # self.contrastive_loss will be defined as a method\n","        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n","        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n","            name=\"c_acc\"\n","        )\n","        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n","        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n","\n","    @property\n","    def metrics(self):\n","        return [\n","            self.contrastive_loss_tracker,\n","            self.contrastive_accuracy,\n","            self.probe_loss_tracker,\n","            self.probe_accuracy,\n","        ]\n","\n","    def contrastive_loss(self, projections_1, projections_2):\n","        # InfoNCE loss (information noise-contrastive estimation)\n","        # NT-Xent loss (normalized temperature-scaled cross entropy)\n","\n","        # Cosine similarity: the dot product of the l2-normalized feature vectors\n","        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n","        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n","        similarities = (\n","            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n","        )\n","\n","        # The similarity between the representations of two augmented views of the\n","        # same image should be higher than their similarity with other views\n","        batch_size = tf.shape(projections_1)[0]\n","        contrastive_labels = tf.range(batch_size)\n","        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n","        self.contrastive_accuracy.update_state(\n","            contrastive_labels, tf.transpose(similarities)\n","        )\n","\n","        # The temperature-scaled similarities are used as logits for cross-entropy\n","        # a symmetrized version of the loss is used here\n","        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n","            contrastive_labels, similarities, from_logits=True\n","        )\n","        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n","            contrastive_labels, tf.transpose(similarities), from_logits=True\n","        )\n","        return (loss_1_2 + loss_2_1) / 2\n","\n","    def train_step(self, data):\n","        labeled_images, labels = data\n","\n","        # Each image is augmented twice, differently\n","        augmented_images_1 = self.contrastive_augmenter(labeled_images)\n","        augmented_images_2 = self.contrastive_augmenter(labeled_images)\n","        with tf.GradientTape() as tape:\n","            features_1 = self.encoder(augmented_images_1, training=True)\n","            features_2 = self.encoder(augmented_images_2, training=True)\n","            # The representations are passed through a projection mlp\n","            projections_1 = self.projection_head(features_1, training=True)\n","            projections_2 = self.projection_head(features_2, training=True)\n","            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n","        gradients = tape.gradient(\n","            contrastive_loss,\n","            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n","        )\n","        self.contrastive_optimizer.apply_gradients(\n","            zip(\n","                gradients,\n","                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n","            )\n","        )\n","        self.contrastive_loss_tracker.update_state(contrastive_loss)\n","\n","        # Labels are only used in evalutation for an on-the-fly logistic regression\n","#         preprocessed_images = self.classification_augmenter(\n","#             labeled_images, training=True\n","#         )\n","        \n","        with tf.GradientTape() as tape:\n","            # the encoder is used in inference mode here to avoid regularization\n","            # and updating the batch normalization paramers if they are used\n","            features = self.encoder(labeled_images, training=False) #preprocessed_images replaced with labeled_images\n","            class_logits = self.linear_probe(features, training=True)\n","            probe_loss = self.probe_loss(labels, class_logits)\n","        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n","        self.probe_optimizer.apply_gradients(\n","            zip(gradients, self.linear_probe.trainable_weights)\n","        )\n","        self.probe_loss_tracker.update_state(probe_loss)\n","        self.probe_accuracy.update_state(labels, class_logits)\n","\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def test_step(self, data):\n","        labeled_images, labels = data\n","\n","        # For testing the components are used with a training=False flag\n","#         preprocessed_images = self.classification_augmenter(\n","#             labeled_images, training=False\n","#         )\n","        features = self.encoder(labeled_images, training=False) #preprocessed_images replaced with labeled_images\n","        class_logits = self.linear_probe(features, training=False)\n","        probe_loss = self.probe_loss(labels, class_logits)\n","        self.probe_loss_tracker.update_state(probe_loss)\n","        self.probe_accuracy.update_state(labels, class_logits)\n","\n","        # Only the probe metrics are logged at test time\n","        return {m.name: m.result() for m in self.metrics[2:]}\n","\n","\n","# Contrastive pretraining\n","pretraining_model = ContrastiveModel()\n","pretraining_model.compile(\n","    contrastive_optimizer=keras.optimizers.Adam(),\n","    probe_optimizer=keras.optimizers.Adam(),\n",")\n","\n","pretraining_history = pretraining_model.fit(train_generator_label, steps_per_epoch=steps_per_epoch, epochs=num_epochs, \n","                                            validation_data=test_generator_label, validation_steps = validation_steps)\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(pretraining_history.history[\"val_p_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T10:24:01.492425Z","iopub.status.busy":"2023-06-23T10:24:01.492042Z","iopub.status.idle":"2023-06-23T10:24:01.916548Z","shell.execute_reply":"2023-06-23T10:24:01.915483Z","shell.execute_reply.started":"2023-06-23T10:24:01.492389Z"},"trusted":true},"outputs":[],"source":["pretraining_model.save_weights('pretraining_model_rotational.h5')\n","with open('pretraining_history_rotational.pkl', 'wb') as file:\n","    pickle.dump(pretraining_history.history, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T10:24:01.919526Z","iopub.status.busy":"2023-06-23T10:24:01.919111Z","iopub.status.idle":"2023-06-23T11:33:17.487929Z","shell.execute_reply":"2023-06-23T11:33:17.486901Z","shell.execute_reply.started":"2023-06-23T10:24:01.919490Z"},"trusted":true},"outputs":[],"source":["finetuning_model_rotation = keras.Sequential(\n","    [\n","        pretraining_model.encoder,\n","        layers.Dense(3, activation='softmax'),\n","    ],\n","    name=\"finetuning_model_rotation\",\n",")\n","finetuning_model_rotation.compile(\n","    optimizer=keras.optimizers.Adam(learning_rate= 1e-4),\n","    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n","    metrics=['acc', metrics.AUC(name='auc')])\n","\n","finetuning_history_rotation = finetuning_model_rotation.fit(train_generator_one_hot, steps_per_epoch=steps_per_epoch, epochs=num_epochs, \n","                                          validation_data=test_generator_one_hot, validation_steps = validation_steps)\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(finetuning_history_rotation.history[\"val_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T11:33:17.491920Z","iopub.status.busy":"2023-06-23T11:33:17.491592Z","iopub.status.idle":"2023-06-23T11:33:17.969310Z","shell.execute_reply":"2023-06-23T11:33:17.968245Z","shell.execute_reply.started":"2023-06-23T11:33:17.491893Z"},"trusted":true},"outputs":[],"source":["finetuning_model_rotation.save_weights('finetuning_model_rotation.h5')\n","# loaded_model = ContrastiveModel()\n","# pretraining_model.load_weights('pretraining_model.h5')\n","with open('finetuning_history_rotation.pkl', 'wb') as file:\n","    pickle.dump(finetuning_history_rotation.history, file)"]},{"cell_type":"markdown","metadata":{},"source":["### With Gaussian blur"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-06-23T12:30:38.757033Z","iopub.status.busy":"2023-06-23T12:30:38.756658Z","iopub.status.idle":"2023-06-23T13:44:21.574196Z","shell.execute_reply":"2023-06-23T13:44:21.573139Z","shell.execute_reply.started":"2023-06-23T12:30:38.757000Z"},"trusted":true},"outputs":[],"source":["# Define the contrastive model with model-subclassing\n","class ContrastiveModel(keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.temperature = temperature\n","        self.contrastive_augmenter = get_augmenter_only_gaussian(**contrastive_augmentation_gaussian)\n","        self.encoder = get_encoder()\n","        # Non-linear MLP as projection head\n","        self.projection_head = keras.Sequential(\n","            [\n","                keras.Input(shape=(width,)),\n","                layers.Dense(width, activation=\"relu\"),\n","                layers.Dense(width),\n","            ],\n","            name=\"projection_head\",\n","        )\n","        # Single dense layer for linear probing\n","        self.linear_probe = keras.Sequential(\n","            [layers.Input(shape=(width,)), layers.Dense(3)], name=\"linear_probe\"\n","        )\n","\n","        self.encoder.summary()\n","        self.projection_head.summary()\n","        self.linear_probe.summary()\n","\n","    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n","        super().compile(**kwargs)\n","\n","        self.contrastive_optimizer = contrastive_optimizer\n","        self.probe_optimizer = probe_optimizer\n","\n","        # self.contrastive_loss will be defined as a method\n","        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n","        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n","            name=\"c_acc\"\n","        )\n","        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n","        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n","\n","    @property\n","    def metrics(self):\n","        return [\n","            self.contrastive_loss_tracker,\n","            self.contrastive_accuracy,\n","            self.probe_loss_tracker,\n","            self.probe_accuracy,\n","        ]\n","\n","    def contrastive_loss(self, projections_1, projections_2):\n","        # InfoNCE loss (information noise-contrastive estimation)\n","        # NT-Xent loss (normalized temperature-scaled cross entropy)\n","\n","        # Cosine similarity: the dot product of the l2-normalized feature vectors\n","        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n","        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n","        similarities = (\n","            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n","        )\n","\n","        # The similarity between the representations of two augmented views of the\n","        # same image should be higher than their similarity with other views\n","        batch_size = tf.shape(projections_1)[0]\n","        contrastive_labels = tf.range(batch_size)\n","        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n","        self.contrastive_accuracy.update_state(\n","            contrastive_labels, tf.transpose(similarities)\n","        )\n","\n","        # The temperature-scaled similarities are used as logits for cross-entropy\n","        # a symmetrized version of the loss is used here\n","        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n","            contrastive_labels, similarities, from_logits=True\n","        )\n","        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n","            contrastive_labels, tf.transpose(similarities), from_logits=True\n","        )\n","        return (loss_1_2 + loss_2_1) / 2\n","\n","    def train_step(self, data):\n","        labeled_images, labels = data\n","\n","        # Each image is augmented twice, differently\n","        augmented_images_1 = self.contrastive_augmenter(labeled_images)\n","        augmented_images_2 = self.contrastive_augmenter(labeled_images)\n","        with tf.GradientTape() as tape:\n","            features_1 = self.encoder(augmented_images_1, training=True)\n","            features_2 = self.encoder(augmented_images_2, training=True)\n","            # The representations are passed through a projection mlp\n","            projections_1 = self.projection_head(features_1, training=True)\n","            projections_2 = self.projection_head(features_2, training=True)\n","            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n","        gradients = tape.gradient(\n","            contrastive_loss,\n","            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n","        )\n","        self.contrastive_optimizer.apply_gradients(\n","            zip(\n","                gradients,\n","                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n","            )\n","        )\n","        self.contrastive_loss_tracker.update_state(contrastive_loss)\n","\n","        # Labels are only used in evalutation for an on-the-fly logistic regression\n","#         preprocessed_images = self.classification_augmenter(\n","#             labeled_images, training=True\n","#         )\n","        \n","        with tf.GradientTape() as tape:\n","            # the encoder is used in inference mode here to avoid regularization\n","            # and updating the batch normalization paramers if they are used\n","            features = self.encoder(labeled_images, training=False) #preprocessed_images replaced with labeled_images\n","            class_logits = self.linear_probe(features, training=True)\n","            probe_loss = self.probe_loss(labels, class_logits)\n","        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n","        self.probe_optimizer.apply_gradients(\n","            zip(gradients, self.linear_probe.trainable_weights)\n","        )\n","        self.probe_loss_tracker.update_state(probe_loss)\n","        self.probe_accuracy.update_state(labels, class_logits)\n","\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def test_step(self, data):\n","        labeled_images, labels = data\n","\n","        # For testing the components are used with a training=False flag\n","#         preprocessed_images = self.classification_augmenter(\n","#             labeled_images, training=False\n","#         )\n","        features = self.encoder(labeled_images, training=False) #preprocessed_images replaced with labeled_images\n","        class_logits = self.linear_probe(features, training=False)\n","        probe_loss = self.probe_loss(labels, class_logits)\n","        self.probe_loss_tracker.update_state(probe_loss)\n","        self.probe_accuracy.update_state(labels, class_logits)\n","\n","        # Only the probe metrics are logged at test time\n","        return {m.name: m.result() for m in self.metrics[2:]}\n","\n","\n","# Contrastive pretraining\n","pretraining_model_gaussian = ContrastiveModel()\n","pretraining_model_gaussian.compile(\n","    contrastive_optimizer=keras.optimizers.Adam(),\n","    probe_optimizer=keras.optimizers.Adam(),\n",")\n","\n","pretraining_history_gaussian = pretraining_model_gaussian.fit(train_generator_label, steps_per_epoch=steps_per_epoch, epochs=num_epochs, \n","                                            validation_data=test_generator_label, validation_steps = validation_steps)\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(pretraining_history_gaussian.history[\"val_p_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T15:06:36.389848Z","iopub.status.busy":"2023-06-23T15:06:36.389384Z","iopub.status.idle":"2023-06-23T15:06:36.895488Z","shell.execute_reply":"2023-06-23T15:06:36.894256Z","shell.execute_reply.started":"2023-06-23T15:06:36.389809Z"},"trusted":true},"outputs":[],"source":["pretraining_model_gaussian.save_weights('pretraining_model_gaussian.h5')\n","with open('pretraining_history_gaussian.pkl', 'wb') as file:\n","    pickle.dump(pretraining_history_gaussian.history, file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T13:51:46.505471Z","iopub.status.busy":"2023-06-23T13:51:46.505093Z","iopub.status.idle":"2023-06-23T14:58:01.266589Z","shell.execute_reply":"2023-06-23T14:58:01.265460Z","shell.execute_reply.started":"2023-06-23T13:51:46.505441Z"},"trusted":true},"outputs":[],"source":["finetuning_model_gaussian = keras.Sequential(\n","    [\n","        pretraining_model_gaussian.encoder,\n","        layers.Dense(3, activation='softmax'),\n","    ],\n","    name=\"finetuning_model\",\n",")\n","finetuning_model_gaussian.compile(\n","    optimizer=keras.optimizers.Adam(learning_rate= 1e-4),\n","    loss=keras.losses.CategoricalCrossentropy(from_logits=False),\n","    metrics=['acc', metrics.AUC(name='auc')])\n","\n","finetuning_history_gaussian = finetuning_model_gaussian.fit(train_generator_one_hot, steps_per_epoch=steps_per_epoch, epochs=num_epochs, \n","                                          validation_data=test_generator_one_hot, validation_steps = validation_steps)\n","print(\n","    \"Maximal validation accuracy: {:.2f}%\".format(\n","        max(finetuning_history_gaussian.history[\"val_acc\"]) * 100\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T15:06:50.212032Z","iopub.status.busy":"2023-06-23T15:06:50.211344Z","iopub.status.idle":"2023-06-23T15:06:50.609367Z","shell.execute_reply":"2023-06-23T15:06:50.608274Z","shell.execute_reply.started":"2023-06-23T15:06:50.211997Z"},"trusted":true},"outputs":[],"source":["finetuning_model_gaussian.save_weights('finetuning_model_gaussian.h5')\n","with open('finetuning_history_gaussian.pkl', 'wb') as file:\n","    pickle.dump(finetuning_history_gaussian.history, file)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T19:05:52.316651Z","iopub.status.busy":"2023-06-23T19:05:52.315849Z","iopub.status.idle":"2023-06-23T19:05:52.323383Z","shell.execute_reply":"2023-06-23T19:05:52.322365Z","shell.execute_reply.started":"2023-06-23T19:05:52.316610Z"},"trusted":true},"outputs":[],"source":["with open(\"history/baseline_history_resnet50.pkl\", \"rb\") as f:\n","    baseline_history = pickle.load(f)\n","with open(\"history/finetuning_history_rotation.pkl\", \"rb\") as f:\n","    finetuning_history_rotation = pickle.load(f)\n","with open(\"history/finetuning_history_gaussian.pkl\", \"rb\") as f:\n","    finetuning_history_gaussian = pickle.load(f)   "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-23T19:05:55.308737Z","iopub.status.busy":"2023-06-23T19:05:55.308367Z","iopub.status.idle":"2023-06-23T19:05:57.321754Z","shell.execute_reply":"2023-06-23T19:05:57.320331Z","shell.execute_reply.started":"2023-06-23T19:05:55.308708Z"},"trusted":true},"outputs":[],"source":["def plot_training_curves(baseline_history, finetuning_history_rotation, finetuning_history_gaussian):\n","    fig, axes = plt.subplots(2, 3, figsize=(20, 9), dpi=100)\n","    \n","    metric_keys = [\"auc\", \"acc\", \"loss\"]\n","    metric_names = [\"auc\", \"acc\", \"loss\"]\n","    \n","    # Plotting for MODEL II Training\n","    for i, (metric_key, metric_name) in enumerate(zip(metric_keys, metric_names)):\n","        axes[0,i].plot(baseline_history[f\"{metric_key}\"], label=\"Supervised ResNet50 Baseline\", linestyle='--', linewidth=2)\n","        axes[0,i].plot(finetuning_history_rotation[f\"{metric_key}\"], label=\"Self-Supervised Finetuning (Rotation Pretext)\", linestyle='--', linewidth=2)\n","        axes[0,i].plot(finetuning_history_gaussian[f\"{metric_key}\"], label=\"Self-Supervised Finetuning (Gaussian Noise Pretext)\", linestyle='--', linewidth=2)\n","\n","        axes[0,i].legend(fontsize='large')\n","        axes[0,i].set_title(f\"MODEL II: Classification TRAIN {metric_name} during training\")\n","        axes[0,i].set_xlabel(\"epochs\")\n","        axes[0,i].set_ylabel(f\"Training {metric_name}\")\n","\n","    # Plotting for MODEL II Validation\n","    for i, (metric_key, metric_name) in enumerate(zip(metric_keys, metric_names)):\n","        axes[1,i].plot(baseline_history[f\"{metric_key}\"], label=\"Supervised ResNet50 Baseline\", linestyle='--', linewidth=2)\n","        axes[1,i].plot(finetuning_history_rotation[f\"val_{metric_key}\"], label=\"Self-Supervised Finetuning (Rotation Pretext)\", linestyle='--', linewidth=2)\n","        axes[1,i].plot(finetuning_history_gaussian[f\"val_{metric_key}\"], label=\"Self-Supervised Finetuning (Gaussian Noise Pretext)\", linestyle='--', linewidth=2)\n","\n","        axes[1,i].legend(fontsize='large')\n","        axes[1,i].set_title(f\"MODEL II: Classification VALIDATION {metric_name} during training\")\n","        axes[1,i].set_xlabel(\"epochs\")\n","        axes[1,i].set_ylabel(f\"Validation {metric_name}\")\n","    \n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_training_curves(baseline_history, finetuning_history_rotation, finetuning_history_gaussian)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:33:44.472575Z","iopub.status.busy":"2023-06-24T09:33:44.472191Z","iopub.status.idle":"2023-06-24T09:33:45.626815Z","shell.execute_reply":"2023-06-24T09:33:45.625767Z","shell.execute_reply.started":"2023-06-24T09:33:44.472546Z"},"trusted":true},"outputs":[],"source":["baseline_model.load_weights('weights/baseline_model_resnet50.h5')\n","finetuning_model_rotation.load_weights('weights/finetuning_model_rotation.h5')\n","finetuning_model_gaussian.load_weights('weights/finetuning_model_gaussian.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:33:47.357508Z","iopub.status.busy":"2023-06-24T09:33:47.356429Z","iopub.status.idle":"2023-06-24T09:33:48.237661Z","shell.execute_reply":"2023-06-24T09:33:48.236575Z","shell.execute_reply.started":"2023-06-24T09:33:47.357470Z"},"trusted":true},"outputs":[],"source":["val_file_paths = []\n","for class_name in classes:\n","    val_class_dir = os.path.join('Model_II_test/' , class_name)\n","    val_file_paths += [os.path.join(val_class_dir, file) for file in os.listdir(val_class_dir)]\n","val_generator = image_generator(val_file_paths, label_type='one_hot', batch_size = batch_size)\n","\n","# baseline_model_results = baseline_model.predict(val_generator, steps=len(val_file_paths), verbose=1)\n","# finetuning_model_rotation_results = finetuning_model_rotation.predict(val_generator, steps=len(val_file_paths), verbose=1)\n","# finetuning_model_gaussian_results = finetuning_model_gaussian.predict(val_generator, steps=len(val_file_paths), verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_batches = len(val_file_paths) // batch_size\n","\n","y_true_baseline = []\n","y_pred_baseline = []\n","y_true_finetuning_rotation = []\n","y_pred_finetuning_rotation = []\n","y_true_finetuning_gaussian = []\n","y_pred_finetuning_gaussian = []\n","\n","for i in range(num_batches):\n","    X_batch, y_batch = next(val_generator)\n","    y_true_baseline.extend(y_batch)\n","    y_pred_baseline.extend(baseline_model.predict(X_batch))\n","    y_true_finetuning_rotation.extend(y_batch)\n","    y_pred_finetuning_rotation.extend(finetuning_model_rotation.predict(X_batch))\n","    y_true_finetuning_gaussian.extend(y_batch)\n","    y_pred_finetuning_gaussian.extend(finetuning_model_gaussian.predict(X_batch))\n","\n","# Here is the extra logic for handling the last smaller batch\n","if len(val_file_paths) % batch_size != 0:\n","    X_batch, y_batch = next(val_generator)\n","    y_true_baseline.extend(y_batch)\n","    y_pred_baseline.extend(baseline_model.predict(X_batch))\n","    y_true_finetuning_rotation.extend(y_batch)\n","    y_pred_finetuning_rotation.extend(finetuning_model_rotation.predict(X_batch))\n","    y_true_finetuning_gaussian.extend(y_batch)\n","    y_pred_finetuning_gaussian.extend(finetuning_model_gaussian.predict(X_batch))\n","\n","y_true_baseline = np.array(y_true_baseline)\n","y_pred_baseline = np.array(y_pred_baseline)\n","y_true_finetuning_rotation = np.array(y_true_finetuning_rotation)\n","y_pred_finetuning_rotation = np.array(y_pred_finetuning_rotation)\n","y_true_finetuning_gaussian = np.array(y_true_finetuning_gaussian)\n","y_pred_finetuning_gaussian = np.array(y_pred_finetuning_gaussian)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:46:14.227232Z","iopub.status.busy":"2023-06-24T09:46:14.226713Z","iopub.status.idle":"2023-06-24T09:46:14.395676Z","shell.execute_reply":"2023-06-24T09:46:14.394539Z","shell.execute_reply.started":"2023-06-24T09:46:14.227197Z"},"trusted":true},"outputs":[],"source":["# Convert one hot encoded labels back to single integer labels for accuracy and classification report\n","y_true_baseline_int = np.argmax(y_true_baseline, axis=1)\n","y_pred_baseline_int = np.argmax(y_pred_baseline, axis=1)\n","y_true_finetuning_rotation_int = np.argmax(y_true_finetuning_rotation, axis=1)\n","y_pred_finetuning_rotation_int = np.argmax(y_pred_finetuning_rotation, axis=1)\n","y_true_finetuning_gaussian_int = np.argmax(y_true_finetuning_gaussian, axis=1)\n","y_pred_finetuning_gaussian_int = np.argmax(y_pred_finetuning_gaussian, axis=1)\n","\n","# Calculate AUC\n","auc_baseline = roc_auc_score(y_true_baseline, y_pred_baseline, average='macro', multi_class='ovr')\n","auc_finetuning_rotation = roc_auc_score(y_true_finetuning_rotation, y_pred_finetuning_rotation, average='macro', multi_class='ovr')\n","auc_finetuning_gaussian = roc_auc_score(y_true_finetuning_gaussian, y_pred_finetuning_gaussian, average='macro', multi_class='ovr')\n","\n","print(\"Separate test set results (MODEL II: Containing 5000 samples per class):\\n\")\n","print(f\"AUC (Baseline): {int(auc_baseline * 1000) / 1000}\")  \n","print(f\"AUC (Finetuning Rotation): {int(auc_finetuning_rotation * 1000) / 1000}\") \n","print(f\"AUC (Finetuning Gaussian): {int(auc_finetuning_gaussian * 1000) / 1000}\") \n","\n","# Calculate accuracy\n","accuracy_baseline = accuracy_score(y_true_baseline_int, y_pred_baseline_int)\n","accuracy_finetuning_rotation = accuracy_score(y_true_finetuning_rotation_int, y_pred_finetuning_rotation_int)\n","accuracy_finetuning_gaussian = accuracy_score(y_true_finetuning_gaussian_int, y_pred_finetuning_gaussian_int)\n","\n","# Print classification report\n","print(\"\\nMODEL II: Classification Report (Baseline):\")\n","print(classification_report(y_true_baseline_int, y_pred_baseline_int))\n","print(\"\\nMODEL II: Classification Report (Finetuning Rotation):\")\n","print(classification_report(y_true_finetuning_rotation_int, y_pred_finetuning_rotation_int))\n","print(\"\\nMODEL II: Classification Report (Finetuning Gaussian):\")\n","print(classification_report(y_true_finetuning_gaussian_int, y_pred_finetuning_gaussian_int))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-24T09:50:29.448366Z","iopub.status.busy":"2023-06-24T09:50:29.447190Z","iopub.status.idle":"2023-06-24T09:50:30.971657Z","shell.execute_reply":"2023-06-24T09:50:30.970700Z","shell.execute_reply.started":"2023-06-24T09:50:29.448296Z"},"trusted":true},"outputs":[],"source":["datasets = {\n","    \"Resnet50 Baseline\": (y_true_baseline, y_pred_baseline),\n","    \"Finetuned with Rotation Pretext\": (y_true_finetuning_rotation, y_pred_finetuning_rotation),\n","    \"Finetuned with Gaussian Noise Pretext\": (y_true_finetuning_gaussian, y_pred_finetuning_gaussian)\n","}\n","\n","plt.figure(figsize=(12, 16)) \n","for idx, (title, (y_true, y_pred)) in enumerate(datasets.items()):\n","    cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n","    plt.subplot(len(datasets), 2, 2*idx+1)\n","    plt.imshow(cm, cmap=plt.cm.Blues)\n","    plt.title(f'MODEL II: Confusion Matrix \\n({title})', fontsize=12)\n","    plt.xlabel('Predicted Labels', fontsize=11)\n","    plt.ylabel('True Labels', fontsize=11)\n","\n","    tick_marks = np.arange(len(class_indices))\n","    plt.xticks(tick_marks, class_indices)\n","    plt.yticks(tick_marks, class_indices, rotation=90)\n","\n","    thresh = cm.max() / 2\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            plt.text(j, i, format(cm[i, j], 'd'),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","\n","    for i in range(3):\n","        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    plt.subplot(len(datasets), 2, 2*idx+2)\n","    for class_name, i in class_indices.items():\n","        plt.plot(fpr[i], tpr[i], label=f\"{class_name} substructure (AUC = {roc_auc[i]:.2f})\")\n","\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.title(f'MODEL II: ROC \\n({title})', fontsize=12)\n","    plt.xlabel('False Positive Rate', fontsize=11)\n","    plt.ylabel('True Positive Rate', fontsize=11)\n","    plt.legend(loc=\"lower right\")\n","\n","plt.subplots_adjust(wspace=0, hspace=0.4)\n","plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":4}
